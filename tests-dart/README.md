# Dart Audio Transcription Test

## Setup
1. Install Dart dependencies: `dart pub get`
2. Start server on port 8080: `python ../src/server.py`
3. Run test: `dart test`

## Audio Format Requirements

### Core Specifications
- **Sample Rate:** 16,000 Hz (16 kHz) - required for both endpoints
- **Channels:** Mono (single channel) - stereo automatically converted
- **Processing Format:** 32-bit floating point (-1.0 to 1.0 range)
- **Minimum Length:** 0.1 seconds (1,600 samples at 16kHz)

### WebSocket `/stream_timestamped` Requirements
- **Input:** Raw Float32 binary data only
- **Chunk Size:** 2 seconds of audio (32,000 samples)
- **Format:** Pre-processed to 16kHz mono float32
- **Streaming:** Continuous chunks of binary float32 data

### HTTP `/transcribe` Requirements  
- **Input:** Uncompressed audio files (WAV, FLAC, etc.) or raw float32 binary
- **Recommended:** WAV format (16-bit or 24-bit, any sample rate)
- **Auto-Processing:** Automatic resampling and mono conversion
- **Supported Formats:** WAV, FLAC, OGG, AIFF (formats supported by libsndfile)
- **Not Supported:** MP3, AAC, or other compressed formats

### ‚ö†Ô∏è Input Format Ranking (by speed/performance):

| Format             | Speed       | Notes                                      |
| ------------------ | ----------- | ------------------------------------------ |
| `float32` raw PCM  | üî• Fastest  | Must match 16kHz mono; ideal for WebSocket |
| WAV (16kHz mono)   | ‚úÖ Fast     | Very common, minor parsing overhead        |
| FLAC               | ‚ö†Ô∏è Moderate  | Lossless but requires decoding             |
| MP3                | üê¢ Slower   | Needs full decoding; variable quality      |
| Stereo input       | ‚ö†Ô∏è Slower    | Requires channel reduction                 |
| Other sample rates | üêå Slowest  | Triggers costly resampling                 |

## Test Audio
- File: `test_audio.wav`
- Format: WAV, 16kHz, mono, 16-bit
- Tests WebSocket transcription with timestamps

## Endpoints Tested

### WebSocket Endpoints
- `/stream` - Returns flat phoneme arrays without timestamps
- `/stream_timestamped` - Returns phonemes with timestamps (used by WebSocket test)

### HTTP Endpoints  
- `POST /transcribe` - Accepts audio file upload, returns phonemes with timestamps (used by HTTP test)

## Transcription Output Format

### JSON Structure
Both endpoints return a JSON array where each element represents a phoneme with timing:
```json
[
  ["phoneme_symbol", start_time, end_time],
  ["phoneme_symbol", start_time, end_time],
  ...
]
```

### Example Output
```json
[
  ["h", 0.123, 0.245],
  ["…õ", 0.245, 0.367], 
  ["l", 0.367, 0.489],
  ["o ä", 0.489, 0.712]
]
```

### Phoneme Format: IPA (International Phonetic Alphabet)
- **Format:** Unicode IPA symbols (not ARPAbet ASCII)
- **Model:** Uses `KoelLabs/xlsr-english-01` with 77 phoneme vocabulary
- **Examples:** `√¶` (ash), `Œ∏` (voiceless th), `√∞` (voiced th), ` É` (sh), `≈ã` (ng), `…π` (r-sound)
- **Source:** Generated by `processor.decode()` in `server.py:68`

### Timing Information
- **start_time:** Float, seconds from audio start
- **end_time:** Float, seconds from audio start  
- **Precision:** Frame-level timing from CTC model
- **Note:** Timestamps may not be perfectly monotonic due to acoustic processing

## How to Call the Endpoints

### WebSocket `/stream_timestamped`
```dart
// Connect to WebSocket
final channel = WebSocketChannel.connect(Uri.parse('ws://localhost:8080/stream_timestamped'));

// Send audio as Float32 binary chunks (2-second chunks = 32,000 samples)
const chunkSize = 32000; // 2 seconds at 16kHz
for (int i = 0; i < audioSamples.length; i += chunkSize) {
  final end = (i + chunkSize < audioSamples.length) ? i + chunkSize : audioSamples.length;
  final chunk = audioSamples.sublist(i, end);
  
  // Convert to Float32 binary data
  final float32Data = Float32List.fromList(chunk);
  final binaryData = float32Data.buffer.asUint8List();
  channel.sink.add(binaryData);
}

// Listen for phoneme responses (IPA format)
channel.stream.listen((data) {
  final phonemes = jsonDecode(data); // [["…õ", 0.245, 0.367], ["l", 0.367, 0.489], ...]
  for (var phoneme in phonemes) {
    print('IPA: ${phoneme[0]}, Start: ${phoneme[1]}s, End: ${phoneme[2]}s');
  }
});
```

### HTTP `POST /transcribe`
```dart
// Create multipart request
final uri = Uri.parse('http://localhost:8080/transcribe');
final request = http.MultipartRequest('POST', uri);

// Add WAV audio file (recommended format)
request.files.add(http.MultipartFile.fromBytes(
  'audio',
  audioBytes,
  filename: 'audio.wav', // Use .wav extension for best compatibility
));

// Send request and get response
final response = await http.Response.fromStream(await request.send());
final phonemes = jsonDecode(response.body); // [["h", 0.123, 0.245], ["…õ", 0.245, 0.367], ...]

// Process IPA phonemes
for (var phoneme in phonemes) {
  String ipaSymbol = phoneme[0];     // IPA symbol (e.g., "…õ", "Œ∏", "√¶")
  double startTime = phoneme[1];     // Start time in seconds
  double endTime = phoneme[2];       // End time in seconds
  print('$ipaSymbol: ${startTime}s ‚Üí ${endTime}s');
}
```

## Test Features
- **Real-time Display**: Shows phonemes as they arrive from server (WebSocket)
- **Timeline View**: Chronologically sorted phoneme sequence
- **Monotonicity Analysis**: Analyzes timestamp ordering patterns
- **Pretty Printing**: Formatted final results with detailed analysis
- **Performance Metrics**: HTTP processing time measurement

## Timestamp Monotonicity Analysis

### Understanding Non-Sequential Timestamps

**IMPORTANT**: Timestamp monotonicity violations are **NORMAL** in speech recognition systems. The test includes detailed analysis to demonstrate this reality.

#### Why Timestamps May Not Be Sequential

1. **Acoustic Confidence-Based Detection**
   - Models detect clear, high-confidence phonemes first
   - Unclear or ambiguous phonemes are identified later with more context
   - Detection order ‚â† temporal order

2. **Chunked Audio Processing**
   - Audio is processed in 2-second chunks
   - Each chunk may detect phonemes from different time periods
   - Cross-chunk phonemes can appear out of temporal sequence

3. **Co-articulation Effects**
   - Phonemes in natural speech overlap and influence each other
   - Some phonemes require surrounding context for accurate identification
   - Temporal boundaries are fuzzy, not discrete

4. **Context-Dependent Recognition**
   - Later phonemes can provide context that disambiguates earlier unclear sounds
   - The model may "back-fill" previously uncertain phonemes

#### Example from Real Output
```
Detection Order:  [o, 1.829s] ‚Üí [k, 1.929s] ‚Üí [e…™, 1.949s] ‚Üí [n, 0.884s]
                                                              ‚Üë Earlier timestamp\!
```

This is **expected behavior** - the 'n' phoneme at 0.884s was detected after the later phonemes due to acoustic processing patterns.

#### Validation Strategy

The test performs **relaxed validation**:
- ‚úÖ **Individual phoneme consistency**: `end_time >= start_time`  
- ‚úÖ **Data format validation**: Proper JSON structure
- ‚ùå **Cross-phoneme monotonicity**: NOT enforced (unrealistic expectation)

The `analyzeTimestampMonotonicity()` function provides:
- Violation count and percentage
- Detailed violation analysis
- Classification of violation types (overlap vs out-of-order)
- Educational output explaining why violations are normal

### Violation Types

1. **out_of_order**: Phoneme detected later but has earlier timestamp
2. **overlap**: Phonemes with overlapping time ranges (common in natural speech)

### Typical Results
- **0-20% violations**: Good quality audio with clear phonemes
- **20-50% violations**: Normal for speech recognition systems  
- **50%+ violations**: Expected for complex audio or chunked processing

This analysis helps developers understand realistic speech recognition behavior vs. idealized sequential expectations.
EOF < /dev/null
